import asyncio
import sys

from dotenv import load_dotenv

from src.core.browser import BrowserManager
from src.core.logger import logger, setup_logger
from src.scrapers.winamax import WinamaxScraper

#cada hijo del elemento ReactVirtualized__Grid__innerScrollContainer es un partido en directo, con sus datos dentro de sus hijos (equipos, cuotas, etc)

async def main() -> None:
    load_dotenv()
    setup_logger("INFO")
    browser = BrowserManager(headless=False)
    scraper = WinamaxScraper(browser)

    try:
        logger.info("ðŸš€ Iniciando el motor de Winamax...")

        # 1. Iniciamos el navegador y navegamos a la web base
        if not await scraper.start():
            logger.error("No se pudo iniciar el scraper.")
            return

        # 2. Navegamos directamente a 'En Vivo' sin loguearnos
        # Si quisieras loguearte, llamarÃ­as a await scraper.login() en su lugar
        if not await scraper.navigate_to_live():
            logger.error("No se pudo navegar a la secciÃ³n en vivo.")
            return

        logger.info("ðŸ“º MONITORIZACIÃ“N ACTIVA ðŸ“º")

        logger.info("Tip: Pulsa Ctrl + C para detener el programa de forma segura.")

        # 4. Obtenemos y mostramos los partidos en vivo
        matches = await scraper.get_live_matches()
        
        if not matches:
            logger.warning("No se encontraron partidos de fÃºtbol en vivo en este momento.")
        else:
            logger.info(f"âš½ Se han encontrado {len(matches)} partidos:")
            for m in matches:
                print(f"[{m.minute or '??'}' ] {m.home_team} {m.score_home} - {m.score_away} {m.away_team}")
                if m.match_url:
                    print(f"    ðŸ”— {m.match_url}")

        # Mantenemos la sesiÃ³n abierta por si el usuario quiere inspeccionar
        if scraper._page:
            # await scraper._page.pause() # Descomentar para inspeccionar manualmente
            pass
            
        await asyncio.Event().wait()
    except Exception as e:
        logger.error(f"Error inesperado: {e}")
    finally:
        logger.info("ðŸ§¹ Limpiando y cerrando pestaÃ±as...")
        await scraper.close()
        await browser.stop()


if __name__ == "__main__":
    sys.setrecursionlimit(2000)
    asyncio.run(main())
